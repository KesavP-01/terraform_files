{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb8b59a-a650-47cb-b00b-91087a2ede36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Overview\n",
    "> Importing rules-based forecast modeling excel file loaded from ADLS volume and creating/replacing tables\n",
    "\n",
    "#### Author: Tucker Campbell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ae2458-8e81-4fdf-94e3-d80908312d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d9b4b8-d425-4bfb-98ce-5acefb6feaa7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Temporarily functions\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Logger Configuration\n",
    "# Design notes:\n",
    "#   - Creates a dta_ingetion.log file that can be set to different levels if we need to debug\n",
    "#   - DEBUG:Detailed information meant for diagnosing issues during development or troubleshooting. typically turned off in production environments.\n",
    "#   - INFO:Confirmation messages that everything is working as expected. \n",
    "#   - WARNING:An indication that something unexpected or concerning happened, but the software is still functioning.\n",
    "#   - ERROR:A serious problem has occurred, indicating a failure in a part of the application. However, the program may still be able to continue running in a limited capacity.\n",
    "#   - CRITICAL:A severe error that indicates the application itself may not be able to continue running. This typically precedes or accompanies a crash or a major outage.\n",
    "# -------------------------------------------------------------------------\n",
    "def configure_logger():\n",
    "    logger = logging.getLogger(\"data_ingestion_logger\")\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all levels at the logger\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        fmt=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    \n",
    "    # Console Handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)  # Only INFO or above goes to console\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File Handler\n",
    "    file_handler = logging.FileHandler(\"data_ingestion.log\", mode='a')\n",
    "    file_handler.setLevel(logging.DEBUG)  # Capture all levels in the file\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Create a global logger instance we can use throughout\n",
    "logger = configure_logger()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Reads Excel or CSV file and converts to pandas dataframe\n",
    "# Design note:\n",
    "#   - Ended up reading everything as string to avoid type errors (weirdness coming from Excel)\n",
    "#   - Intentionally left column_types as parameter here but it is ignored in the function\n",
    "# -------------------------------------------------------------------------\n",
    "def read_excel_or_csv(filename, worksheet=None, skiprows=0):\n",
    "    \"\"\"\n",
    "    Reads an Excel or CSV file using pandas.\n",
    "      - If `worksheet` is provided, attempts to read a specific sheet (Excel file).\n",
    "      - If the file is .csv, `worksheet` will be ignored and the CSV is read directly.\n",
    "      - skiprows can be used to skip initial rows.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Reading file '{filename}', worksheet='{worksheet}', skiprows={skiprows}\")\n",
    "    ext = os.path.splitext(filename)[1].lower()\n",
    "    try:\n",
    "        if ext in ['.xlsx', '.xls']:\n",
    "            # Read a specific worksheet (if provided) from an Excel file\n",
    "            df = pd.read_excel(filename, sheet_name=worksheet, skiprows=skiprows, dtype=str)\n",
    "            logger.debug(f\"DataFrame read from Excel. Columns: {df.columns}\")\n",
    "        elif ext == '.csv':\n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(filename, skiprows=skiprows, dtype=str)\n",
    "            logger.debug(f\"DataFrame read from CSV. Columns: {df.columns}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error reading file '{filename}' with sheet '{worksheet}': {e}\")\n",
    "        raise RuntimeError(f\"Error reading file '{filename}' with sheet '{worksheet}'\") from e\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Takes the pandas dataframe and converts to specific data types as specified \n",
    "# in the config JSON\n",
    "# -------------------------------------------------------------------------\n",
    "def convert_to_expected_types(df, column_types={}):  # pragma: no cover\n",
    "    \"\"\"\n",
    "    Converts the pandas DataFrame to the expected types given in the column_types dictionary from the config file.\n",
    "\n",
    "    :param df: The pandas DataFrame to convert.\n",
    "    :param column_types: A dictionary mapping column names or indices to their expected data types.\n",
    "    :return: The DataFrame with converted data types.\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting columns to expected types.\")\n",
    "\n",
    "    for key, dtype_str in column_types.items():\n",
    "        # Determine if input is using a column name or index\n",
    "        if isinstance(key, str) and key in df.columns:\n",
    "            col_name = key\n",
    "        elif isinstance(key, int) and 0 <= key < len(df.columns):\n",
    "            col_name = df.columns[key]\n",
    "        else:\n",
    "            logger.warning(f\"Key '{key}' not found in DataFrame columns. Skipping conversion.\")\n",
    "            continue\n",
    "\n",
    "        logger.debug(f\"Converting column '{col_name}' to {dtype_str}.\")\n",
    "        try:\n",
    "            if dtype_str in (\"datetime\", \"date\"):\n",
    "                df.loc[:, col_name] = pd.to_datetime(df.loc[:, col_name], errors=\"coerce\")\n",
    "            elif dtype_str in (\"int\", \"float\"):\n",
    "                df.loc[:, col_name] = pd.to_numeric(df.loc[:, col_name], errors=\"coerce\")\n",
    "            else:\n",
    "                df.loc[:, col_name] = df.loc[:, col_name].astype(dtype_str, errors=\"ignore\")\n",
    "            logger.info(f\"Successfully converted column '{col_name}' to {dtype_str}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to convert column '{col_name}' to {dtype_str}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Main Processing Functions for each individual file\n",
    "# -------------------------------------------------------------------------\n",
    "def process_file(config_entry):\n",
    "    \"\"\"\n",
    "    config_entry = Json configurationfile\n",
    "    Steps:\n",
    "      1. Read each worksheet (or CSV) from the file\n",
    "      2. Clean columns (standard naming)\n",
    "      3. Add a 'source_file' column\n",
    "      4. Basic cleanup (fillna(0))\n",
    "      5. Create Spark DataFrame\n",
    "      6. Write to Databricks table\n",
    "    \"\"\"\n",
    "    #volume = config_entry.get(\"volume\")\n",
    "    filename = config_entry.get(\"filename\")\n",
    "    worksheets_info = config_entry.get(\"worksheets\", [])\n",
    "\n",
    "    if not filename or not worksheets_info:\n",
    "        logger.error(\"Config entry must contain 'filename' and a non-empty 'worksheets' list.\")\n",
    "        raise ValueError(\"Config entry must contain 'filename' and a non-empty 'worksheets' list.\")\n",
    "\n",
    "    for sheet_info in worksheets_info:\n",
    "        # worksheet = sheet_info.get(\"worksheet\", None)\n",
    "        worksheet = sheet_info.get(\"worksheet\", 0) # changed default to 0 which is recognized by read_excel\n",
    "        skiprows = sheet_info.get(\"skiprows\", 0)\n",
    "        table_name = sheet_info.get(\"table_name\")\n",
    "        column_types = sheet_info.get(\"column_types\", {})\n",
    "\n",
    "        if not table_name:\n",
    "            logger.error(\"Each worksheet info dict must include a 'table_name'.\")\n",
    "            raise ValueError(\"Each worksheet info dict must include a 'table_name'.\")\n",
    "\n",
    "        logger.info(\n",
    "            f\"Processing file='{filename}', worksheet='{worksheet}', \"\n",
    "            f\"skiprows={skiprows}, table='{table_name}', column_types={column_types}\"\n",
    "        )\n",
    "\n",
    "        # 1) Read data into a pandas DataFrame\n",
    "        df = read_excel_or_csv(\n",
    "            filename,\n",
    "            worksheet=worksheet,\n",
    "            skiprows=skiprows\n",
    "        )\n",
    "\n",
    "        # 2) Convert columns to expected types\n",
    "        df = convert_to_expected_types(df, column_types=column_types)\n",
    "        logger.debug(f\"DataFrame dtypes after conversion:\\n{df.dtypes}\")\n",
    "\n",
    "        # 3) Fix column names\n",
    "        df.columns = fix_column_names(df.columns)\n",
    "\n",
    "        # 4) Add 'source_file' and dttm column\n",
    "        df['source_file'] = os.path.basename(filename)\n",
    "        df['create_dttm'] = pd.to_datetime(dt.datetime.now(pytz.timezone('US/Pacific')))\n",
    "        df['modify_dttm'] = pd.to_datetime(dt.datetime.now(pytz.timezone('US/Pacific')))\n",
    "\n",
    "        # 5) Basic cleanup (fillna(0))\n",
    "        # This could be extracted to a separate function if we find the need for it\n",
    "        df.fillna(0, inplace=True)\n",
    "\n",
    "        # For debugging in a notebook environment, you might show a sample:\n",
    "        logger.debug(f\"Sample data:\\n{df.head(15)}\")\n",
    "\n",
    "        # 6) Convert to Spark DataFrame\n",
    "        try:\n",
    "            spark_df = spark.createDataFrame(df)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error creating Spark DataFrame for '{filename}', worksheet '{worksheet}': {e}\")\n",
    "            raise RuntimeError(f\"Error creating Spark DataFrame for file '{filename}' worksheet '{worksheet}'\") from e\n",
    "\n",
    "        # 7) Write to Databricks table\n",
    "        logger.info(f\"Writing to Databricks table: {table_name}\")\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            spark_df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            logger.info(f\"Successfully wrote data to table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to write to table '{table_name}': {e}\")\n",
    "            raise RuntimeError(f\"Failed to write to table '{table_name}'\") from e\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Step 1: Called from main. Iterates over the list of files passed on the JSON\n",
    "# ---------------------------------------------------------------------------\n",
    "def process_files(config_json):\n",
    "    \"\"\"\n",
    "    Main function to process multiple files based on the provided JSON-like config.\n",
    "    \"\"\"\n",
    "    if isinstance(config_json, str):\n",
    "        config_data = json.loads(config_json)\n",
    "    else:\n",
    "        config_data = config_json\n",
    "\n",
    "    files_list = config_data.get(\"files\", [])\n",
    "    if not files_list:\n",
    "        logger.error(\"No 'files' key found in provided config JSON.\")\n",
    "        raise ValueError(\"No 'files' key found in provided config JSON.\")\n",
    "\n",
    "    for entry in files_list:\n",
    "        process_file(entry)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Need to cleanup then name of the columns from Excel\n",
    "# Design notes:\n",
    "#    - Extracted as a separate function so that we can adjust as needed\n",
    "# -------------------------------------------------------------------------\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Fix column names to meet Databricks standards:\n",
    "      - Add space to words with camel case (longer than 2 characters)\n",
    "      - Remove any invalid characters\n",
    "      - Convert to lowercase\n",
    "      - Remove leading/trailing spaces\n",
    "      - Remove extra spaces between words\n",
    "      - Replace spaces with underscores\n",
    "    \"\"\"\n",
    "    logger.debug(\"Fixing column names to meet Databricks standards.\")\n",
    "    fixed_cols = []\n",
    "    for col in columns:\n",
    "        # Add space to words with camel case (longer than 2 characters)\n",
    "        col_fixed = re.sub(r'(?<!^)(?=[A-Z][a-z]{2,})', ' ', col)\n",
    "        # replace % for percentage\n",
    "        col_fixed = col_fixed.replace('%', 'percentage')\n",
    "        # remove any character that is not alphanumeric or underscore or space\n",
    "        col_fixed = re.sub(r'[^a-zA-Z0-9_\\ ]', '', col_fixed)\n",
    "        # lower, remove leading/trailing spaces\n",
    "        col_fixed = col_fixed.strip('').lower()\n",
    "        # remove extra spaces between words\n",
    "        col_fixed = re.sub(r'\\s+', ' ', col_fixed)\n",
    "        # replace spaces with underscores\n",
    "        col_fixed = col_fixed.replace(' ', '_')\n",
    "        fixed_cols.append(col_fixed)\n",
    "\n",
    "    logger.debug(f\"Original columns: {list(columns)}\")\n",
    "    logger.debug(f\"Fixed columns: {fixed_cols}\")\n",
    "    return fixed_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e553aa-9b40-476d-acbb-c5ea5209f885",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Source File Paths"
    }
   },
   "outputs": [],
   "source": [
    "local_lib = os.getcwd()\n",
    "\n",
    "print(f\"local_lib: {local_lib}\")\n",
    "print(\"File paths have been set.\")\n",
    "sys.path.append(local_lib)\n",
    "\n",
    "sys.path.append(local_lib)\n",
    "#import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22114de0-36fd-499d-b367-e2058b142513",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "env"
    }
   },
   "outputs": [],
   "source": [
    "env = dbutils.secrets.get(scope=\"CommercialAnalytics\", key=\"env-databricks\")\n",
    "if env:\n",
    "  db_env = \"_\" + env\n",
    "else:\n",
    "  db_env = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd527b42-33c7-4735-8db6-f69dd70375ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set catalog / schema"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE business_revenuemanagement{db_env}.an_revenuemanagement_ods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e47a20-d91c-42e9-bb6c-38446d37d4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba887abe-97c4-4f39-b23b-1d927682daae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "get latest file + set column types"
    }
   },
   "outputs": [],
   "source": [
    "# hard coded path for CICD testing\n",
    "latest_file = f'/Volumes/business_revenuemanagement{db_env}/an_revenuemanagement_ods/uploads/revenue_forecast/Rules-Based Post-MVP Review v2.xlsx'\n",
    "# latest_file = fn.get_latest_file(revenue_forecast_directory)\n",
    "col_types = {\n",
    "    \"DEP_MONTH\": \"date\",\n",
    "    \"REGION\": \"str\",\n",
    "    \"NDOD\": \"str\",\n",
    "    \"MONTH\": \"int\",\n",
    "    \"MILES\": \"int\",\n",
    "    \"PROXY_MKT\": \"str\",\n",
    "    \"OPER_CARR\": \"str\",\n",
    "    \"STATS_ASM\": \"int\",\n",
    "    \"PY_ASM\": \"int\",\n",
    "    \"YOY_ASM\": \"float\",\n",
    "    \"PY_RPM\": \"float\",\n",
    "    \"PY_REV\": \"float\",\n",
    "    \"PY_CRASM\": \"float\",\n",
    "    \"IMP_CAP\": \"float\",\n",
    "    \"YOY_OACAP\": \"float\",\n",
    "    \"IMP_OACAP\": \"float\",\n",
    "    \"YOY_OAFARE\": \"float\",\n",
    "    \"IMP_OAFARE\": \"float\",\n",
    "    \"1MO_AIRLINESPEND\": \"float\",\n",
    "    \"IMP_AIRLINESPEND\": \"float\",\n",
    "    \"IMP_TOTAL_CRASM\": \"float\",\n",
    "    \"FCST_CRASM\": \"float\",\n",
    "    \"FCST_REV\": \"float\",\n",
    "    \"YOY_REV\": \"float\",\n",
    "    \"REV_ADJ\": \"float\",\n",
    "    \"ADJ_FCST_REV\": \"float\",\n",
    "    \"ADJ_FCST_CRASM\": \"float\"\n",
    "}\n",
    "\n",
    "#fn.logger.info(f\"Getting path to latest file in directory: {latest_file}\")\n",
    "logger.info(f\"Getting path to latest file in directory: {latest_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5ebec3-5029-46e1-b06d-dd3cf34b6f77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "excel config"
    }
   },
   "outputs": [],
   "source": [
    "config_json = {\n",
    "  \"files\": [\n",
    "    {\n",
    "      \"filename\": latest_file,\n",
    "      \"worksheets\": [\n",
    "        {\n",
    "          \"worksheet\": \"FORECAST\",\n",
    "          \"skiprows\": 0,\n",
    "          \"table_name\": \"cicd_rules_based_forecast_test_stg\",\n",
    "          \"column_types\": col_types\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f63ed7-d0c9-4aa2-a145-074c1e04091c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "process excel"
    }
   },
   "outputs": [],
   "source": [
    "# Process the files\n",
    "if 'rules-based' in latest_file.lower():\n",
    "  #fn.process_files(config_json)\n",
    "  process_files(config_json)\n",
    "else:\n",
    "  #fn.logger.error(\"Latest file is not Rules Based forecast input\")\n",
    "  logger.error(\"Latest file is not Rules Based forecast input\")\n",
    "  dbutils.jobs.taskValues.set(key = \"pass_condition\", value = \"false\")\n",
    "  dbutils.notebook.exit(\"Aborting job task since condition was not met.\")\n",
    "  # raise ValueError(\"Latest file is not Rules Based forecast input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f900e20e-d173-4f85-b053-8d759360d69a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "preview data"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM cicd_rules_based_forecast_test_stg LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3aeae9-a060-4133-826a-55c682548890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2817798-8ca3-4b0e-a34b-8b497208b453",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create base table if not exists"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- create table if not exists cicd_rules_based_forecast_test as\n",
    "-- select *\n",
    "-- from cicd_rules_based_forecast_test_stg;\n",
    "\n",
    "create table if not exists cicd_rules_based_forecast_test (\n",
    "    dep_month date,\n",
    "    region string,\n",
    "    ndod string,\n",
    "    month bigint,\n",
    "    miles double,\n",
    "    proxy_mkt string,\n",
    "    oper_carr string,\n",
    "    stats_asm double,\n",
    "    py_asm bigint,\n",
    "    yoy_asm double,\n",
    "    py_rpm bigint,\n",
    "    py_rev double,\n",
    "    py_crasm double,\n",
    "    imp_cap double,\n",
    "    yoy_oacap double,\n",
    "    imp_oacap double,\n",
    "    yoy_oafare double,\n",
    "    imp_oafare double,\n",
    "    1mo_airlinespend double,\n",
    "    imp_airlinespend double,\n",
    "    imp_total_crasm double,\n",
    "    fcst_crasm double,\n",
    "    fcst_rev double,\n",
    "    yoy_rev double,\n",
    "    rev_adj double,\n",
    "    adj_fcst_rev double,\n",
    "    adj_fcst_crasm double,\n",
    "    source_file string,\n",
    "    create_dttm timestamp,\n",
    "    modify_dttm timestamp\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab93e850-61ca-477b-9f92-14fd9948367d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "insert staged data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into cicd_rules_based_forecast_test\n",
    "select *\n",
    "from cicd_rules_based_forecast_test_stg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de602294-13ad-4647-aadf-0fa03d1ff40e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "drop stage table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists cicd_rules_based_forecast_test_stg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb20cac-ca71-4035-8716-816d2d2883b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "set job value"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key = \"pass_condition\", value = \"true\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3461764014833373,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "rulesbased_forecast_excel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
